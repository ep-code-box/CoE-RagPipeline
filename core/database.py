"""
CoE-RagPipeline ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜
CoE-Backendì™€ í˜¸í™˜ë˜ëŠ” í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import os
import json
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, JSON, Float, ForeignKey, Enum, DECIMAL, inspect
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from datetime import datetime
from dotenv import load_dotenv
import enum

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DB_HOST = os.getenv("DB_HOST", "mariadb")
DB_PORT = os.getenv("DB_PORT", "3306")
DB_USER = os.getenv("DB_USER", "coe_user")
DB_PASSWORD = os.getenv("DB_PASSWORD", "coe_password")
DB_NAME = os.getenv("DB_NAME", "coe_db")

# MariaDB ì—°ê²° URL
DATABASE_URL = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# SQLAlchemy ì—”ì§„ ë° ì„¸ì…˜ ì„¤ì • (ìµœì í™”)
engine = create_engine(
    DATABASE_URL, 
    echo=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” Falseë¡œ ì„¤ì •
    pool_pre_ping=True, 
    pool_recycle=300,
    pool_size=10,  # ì—°ê²° í’€ í¬ê¸°
    max_overflow=20,  # ìµœëŒ€ ì˜¤ë²„í”Œë¡œìš° ì—°ê²° ìˆ˜
    pool_timeout=30,  # ì—°ê²° ëŒ€ê¸° ì‹œê°„
    connect_args={
        "charset": "utf8mb4",
        "connect_timeout": 10,
        "read_timeout": 30,
        "write_timeout": 30
    }
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Enum ì •ì˜ (CoE-Backendì™€ ë™ì¼)
class AnalysisStatus(enum.Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

class RepositoryStatus(enum.Enum):
    PENDING = "PENDING"
    CLONING = "CLONING"
    ANALYZING = "ANALYZING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

class DependencyType(enum.Enum):
    FRAMEWORK = "FRAMEWORK"
    LIBRARY = "LIBRARY"
    TOOL = "TOOL"
    LANGUAGE = "LANGUAGE"

class DocumentType(enum.Enum):
    README = "README"
    API_DOC = "API_DOC"
    WIKI = "WIKI"
    CHANGELOG = "CHANGELOG"
    CONTRIBUTING = "CONTRIBUTING"
    OTHER = "OTHER"

class SourceType(enum.Enum):
    CODE = "CODE"
    DOCUMENT = "DOCUMENT"
    AST_NODE = "AST_NODE"

class StandardType(enum.Enum):
    CODING_STYLE = "CODING_STYLE"
    ARCHITECTURE_PATTERN = "ARCHITECTURE_PATTERN"
    COMMON_FUNCTIONS = "COMMON_FUNCTIONS"
    BEST_PRACTICES = "BEST_PRACTICES"

class HTTPMethod(enum.Enum):
    GET = "GET"
    POST = "POST"
    PUT = "PUT"
    DELETE = "DELETE"
    PATCH = "PATCH"

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜ (CoE-Backendì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)

# ë¶„ì„ ìš”ì²­ í…Œì´ë¸”
class AnalysisRequest(Base):
    __tablename__ = "analysis_requests"
    extend_existing=True

    id = Column(Integer, primary_key=True, index=True)
    analysis_id = Column(String(36), unique=True, index=True, nullable=False)
    status = Column(Enum(AnalysisStatus), default=AnalysisStatus.PENDING)
    repositories = Column(JSON, nullable=False)
    include_ast = Column(Boolean, default=True)
    include_tech_spec = Column(Boolean, default=True)
    include_correlation = Column(Boolean, default=True)
    group_name = Column(String(255), nullable=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)
    error_message = Column(Text, nullable=True)
    
    # ê´€ê³„ ì„¤ì •
    repository_analyses = relationship("RepositoryAnalysis", back_populates="analysis_request", cascade="all, delete-orphan")
    correlation_analyses = relationship("CorrelationAnalysis", back_populates="analysis_request", cascade="all, delete-orphan")
    development_standards = relationship("DevelopmentStandard", back_populates="analysis_request", cascade="all, delete-orphan")

# ì½”ë“œ íŒŒì¼ ì •ë³´ í…Œì´ë¸”
class CodeFile(Base):
    __tablename__ = "code_files"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    repository_analysis_id = Column(Integer, ForeignKey("repository_analyses.id"), nullable=False)
    file_path = Column(String(1000), nullable=False)
    file_name = Column(String(255), nullable=False)
    file_size = Column(Integer, default=0)
    language = Column(String(50))
    complexity_score = Column(DECIMAL(5, 2))
    last_modified = Column(DateTime)
    file_hash = Column(String(64))
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    repository_analysis = relationship("RepositoryAnalysis", back_populates="code_files")
    ast_nodes = relationship("ASTNode", back_populates="code_file", cascade="all, delete-orphan")

# AST ë…¸ë“œ í…Œì´ë¸”
class ASTNode(Base):
    __tablename__ = "ast_nodes"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    code_file_id = Column(Integer, ForeignKey("code_files.id"), nullable=False)
    node_type = Column(String(100), nullable=False)
    node_name = Column(String(255))
    line_start = Column(Integer)
    line_end = Column(Integer)
    parent_id = Column(Integer, ForeignKey("ast_nodes.id"))
    node_metadata = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    code_file = relationship("CodeFile", back_populates="ast_nodes")
    parent = relationship("ASTNode", remote_side=[id])

# ë ˆí¬ì§€í† ë¦¬ ê°„ ì—°ê´€ë„ ë¶„ì„ í…Œì´ë¸”
class CorrelationAnalysis(Base):
    __tablename__ = "correlation_analyses"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    analysis_id = Column(String(36), ForeignKey("analysis_requests.analysis_id"), nullable=False)
    repository1_id = Column(Integer, ForeignKey("repository_analyses.id"), nullable=False)
    repository2_id = Column(Integer, ForeignKey("repository_analyses.id"), nullable=False)
    common_dependencies = Column(JSON)
    similar_patterns = Column(JSON)
    architecture_similarity = Column(DECIMAL(5, 4), default=0.0000)
    shared_technologies = Column(JSON)
    similarity_score = Column(DECIMAL(5, 4), default=0.0000)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    analysis_request = relationship("AnalysisRequest", back_populates="correlation_analyses")

# ë²¡í„° ì„ë² ë”© ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
class VectorEmbedding(Base):
    __tablename__ = "vector_embeddings"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    source_type = Column(Enum(SourceType), nullable=False)
    source_id = Column(Integer, nullable=False)
    chunk_id = Column(String(100), nullable=False)
    collection_name = Column(String(255), nullable=False)
    embedding_model = Column(String(100), default="default")
    chunk_text = Column(Text)
    node_metadata = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)

# ê°œë°œ í‘œì¤€ ë¬¸ì„œ í…Œì´ë¸”
class DevelopmentStandard(Base):
    __tablename__ = "development_standards"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    analysis_id = Column(String(36), ForeignKey("analysis_requests.analysis_id"), nullable=False)
    standard_type = Column(Enum(StandardType), nullable=False)
    title = Column(String(500), nullable=False)
    content = Column(Text, nullable=False)
    examples = Column(JSON)
    recommendations = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    analysis_request = relationship("AnalysisRequest", back_populates="development_standards")

# ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ RAG ë¶„ì„ ê²°ê³¼ í…Œì´ë¸” (ê¸°ì¡´ config/database.pyì™€ í˜¸í™˜)
class RagAnalysisResult(Base):
    __tablename__ = "rag_analysis_results"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    analysis_id = Column(String(255), unique=True, index=True, nullable=False)
    git_url = Column(String(500), index=True, nullable=False)
    analysis_date = Column(DateTime, default=datetime.utcnow, nullable=False)
    status = Column(Enum(AnalysisStatus), default=AnalysisStatus.PENDING)
    repository_count = Column(Integer, default=0)
    total_files = Column(Integer, default=0)
    total_lines_of_code = Column(Integer, default=0)
    
    # ë¶„ì„ ê²°ê³¼ ë°ì´í„° (JSON í˜•íƒœë¡œ ì €ì¥)
    repositories_data = Column(Text, nullable=True)
    correlation_data = Column(Text, nullable=True)
    tech_specs_summary = Column(Text, nullable=True)
    
    # ë©”íƒ€ë°ì´í„°
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)
    error_message = Column(Text, nullable=True)
    
    # ì¸ë±ìŠ¤ ì¶”ê°€
    __table_args__ = (
        {'mysql_charset': 'utf8mb4'},
    )

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜ (CoE-Backendì™€ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©)

# ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
class RepositoryAnalysis(Base):
    __tablename__ = "repository_analyses"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    analysis_id = Column(String(36), ForeignKey("analysis_requests.analysis_id"), nullable=False)
    repository_url = Column(String(500), nullable=False)
    repository_name = Column(String(255))
    branch = Column(String(100), default="main")
    clone_path = Column(String(500))
    status = Column(Enum(RepositoryStatus), default=RepositoryStatus.PENDING)
    # Commit ì •ë³´ í•„ë“œ ì¶”ê°€
    commit_hash = Column(String(40), nullable=True, index=True)  # Git commit hash (SHA-1)
    commit_date = Column(DateTime, nullable=True)
    commit_author = Column(String(255), nullable=True)
    commit_message = Column(Text, nullable=True)
    files_count = Column(Integer, default=0)
    lines_of_code = Column(Integer, default=0)
    languages = Column(JSON)
    frameworks = Column(JSON)
    dependencies = Column(JSON)
    ast_data = Column(Text)
    tech_specs = Column(JSON)
    code_metrics = Column(JSON)
    documentation_files = Column(JSON)
    config_files = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    analysis_request = relationship("AnalysisRequest", back_populates="repository_analyses")
    code_files = relationship("CodeFile", back_populates="repository_analysis", cascade="all, delete-orphan")
    tech_dependencies = relationship("TechDependency", back_populates="repository_analysis", cascade="all, delete-orphan")
    document_analyses = relationship("DocumentAnalysis", back_populates="repository_analysis", cascade="all, delete-orphan")



# ê¸°ìˆ  ìŠ¤íƒ ë° ì˜ì¡´ì„± í…Œì´ë¸”
class TechDependency(Base):
    __tablename__ = "tech_dependencies"
    
    id = Column(Integer, primary_key=True, index=True)
    repository_analysis_id = Column(Integer, ForeignKey("repository_analyses.id"), nullable=False)
    dependency_type = Column(Enum(DependencyType), nullable=False)
    name = Column(String(255), nullable=False)
    version = Column(String(100))
    package_manager = Column(String(50))
    is_dev_dependency = Column(Boolean, default=False)
    license = Column(String(100))
    vulnerability_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    repository_analysis = relationship("RepositoryAnalysis", back_populates="tech_dependencies")

# ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
class DocumentAnalysis(Base):
    __tablename__ = "document_analyses"
    extend_existing=True
    
    id = Column(Integer, primary_key=True, index=True)
    repository_analysis_id = Column(Integer, ForeignKey("repository_analyses.id"), nullable=False)
    document_path = Column(String(1000), nullable=False)
    document_type = Column(Enum(DocumentType), default=DocumentType.OTHER)
    title = Column(String(500))
    content = Column(Text)
    extracted_sections = Column(JSON)
    code_examples = Column(JSON)
    api_endpoints = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ê´€ê³„ ì„¤ì •
    repository_analysis = relationship("RepositoryAnalysis", back_populates="document_analyses")

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì˜ì¡´ì„±
def get_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
def test_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´ ì—°ê²° í…ŒìŠ¤íŠ¸
        test_url = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}"
        test_engine = create_engine(test_url)
        
        with test_engine.connect() as connection:
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            from sqlalchemy import text
            connection.execute(text(f"CREATE DATABASE IF NOT EXISTS {DB_NAME}"))
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{DB_NAME}' ìƒì„±/í™•ì¸ ì™„ë£Œ")
        
        # ì´ì œ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
        with engine.connect() as connection:
            result = connection.execute(text("SELECT 1"))
            print("âœ… MariaDB ì—°ê²° ì„±ê³µ!")
            return True
    except Exception as e:
        print(f"âŒ MariaDB ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ ì¶”ì  (í…Œì´ë¸” í™•ì¸)
def _is_database_initialized():
    """ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        inspector = inspect(engine)
        # í•„ìˆ˜ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì´ˆê¸°í™” ì—¬ë¶€ íŒë‹¨
        # 'rag_analysis_results'ëŠ” ë°±ì›Œë“œ í˜¸í™˜ìš©ìœ¼ë¡œ ì„ íƒ ì‚¬í•­ì´ë¯€ë¡œ ë¶€íŠ¸ ì¡°ê±´ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        required_tables = {'analysis_requests', 'repository_analyses'}
        existing_tables = set(inspector.get_table_names())
        print(f"ğŸ” í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {existing_tables}")
        return required_tables.issubset(existing_tables)
    except Exception as e:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ ë“± ì˜ˆì™¸ ë°œìƒ ì‹œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        print(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì´ˆê¸°í™” í•„ìš” ê°€ëŠ¥ì„±): {e}")
        return False

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°
    if _is_database_initialized():
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
        
    print("ğŸ”„ RAG Pipeline ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_connection():
        return False
    
    # Alembicì´ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” í…Œì´ë¸”ì„ ì§ì ‘ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # create_tables()
    
    # ì´ˆê¸°í™” ì™„ë£Œ í›„ ë‹¤ì‹œ í™•ì¸
    if not _is_database_initialized():
        print("âŒ ì´ˆê¸°í™” í›„ì—ë„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
        
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True

# ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ë°±ì›Œë“œ í˜¸í™˜ì„±)
def save_analysis_to_db(analysis_result):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        db = SessionLocal()
        
        # Git URL ì¶”ì¶œ (ì²« ë²ˆì§¸ ë ˆí¬ì§€í† ë¦¬ì˜ URL ì‚¬ìš©)
        git_url = ""
        if hasattr(analysis_result, 'repositories') and analysis_result.repositories:
            if hasattr(analysis_result.repositories[0], 'repository'):
                git_url = str(analysis_result.repositories[0].repository.url)
            else:
                git_url = "unknown"
        
        # í†µê³„ ê³„ì‚°
        total_files = 0
        total_lines = 0
        if hasattr(analysis_result, 'repositories') and analysis_result.repositories:
            total_files = sum(len(getattr(repo, 'files', [])) for repo in analysis_result.repositories)
            total_lines = sum(getattr(getattr(repo, 'code_metrics', None), 'lines_of_code', 0) for repo in analysis_result.repositories)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ìƒì„±
        db_record = RagAnalysisResult(
            analysis_id=analysis_result.analysis_id,
            git_url=git_url,
            analysis_date=analysis_result.created_at,
            status=analysis_result.status,
            repository_count=len(getattr(analysis_result, 'repositories', [])),
            total_files=total_files,
            total_lines_of_code=total_lines,
            repositories_data=json.dumps([repo.model_dump() if hasattr(repo, 'model_dump') else str(repo) for repo in getattr(analysis_result, 'repositories', [])], default=str, ensure_ascii=False),
            correlation_data=json.dumps(analysis_result.correlation_analysis.model_dump() if hasattr(analysis_result, 'correlation_analysis') and analysis_result.correlation_analysis else None, default=str, ensure_ascii=False),
            tech_specs_summary=json.dumps([], default=str, ensure_ascii=False),
            created_at=analysis_result.created_at,
            completed_at=getattr(analysis_result, 'completed_at', None),
            error_message=getattr(analysis_result, 'error_message', None)
        )
        
        # ê¸°ì¡´ ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        existing = db.query(RagAnalysisResult).filter(RagAnalysisResult.analysis_id == analysis_result.analysis_id).first()
        if existing:
            # ì—…ë°ì´íŠ¸
            for key, value in db_record.__dict__.items():
                if not key.startswith('_'):
                    setattr(existing, key, value)
            db.commit()
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {analysis_result.analysis_id}")
        else:
            # ìƒˆë¡œ ìƒì„±
            db.add(db_record)
            db.commit()
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {analysis_result.analysis_id}")
        
        db.close()
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        if 'db' in locals():
            db.rollback()
            db.close()
        return False

# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
def create_tables():
    """ë°ì´í„°ë² ì´ìŠ¤ì— ëª¨ë“  í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        # Baseì— ì •ì˜ëœ ëª¨ë“  í…Œì´ë¸”ì„ ìƒì„±
        Base.metadata.create_all(bind=engine)
        print("âœ… ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ í™•ì¸ ë˜ëŠ” ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # (ì„ íƒ ì‚¬í•­) ê° í…Œì´ë¸” ìƒì„± ì—¬ë¶€ í™•ì¸ ë¡œê·¸
        inspector = inspect(engine)
        table_names = inspector.get_table_names()
        print(f"ğŸ” í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {table_names}")
        
    except Exception as e:
        print(f"âŒ í…Œì´ë¸” ìƒì„± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ ì¶”ì  (í…Œì´ë¸” í™•ì¸)
def _is_database_initialized():
    """ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        inspector = inspect(engine)
        # í•„ìˆ˜ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì´ˆê¸°í™” ì—¬ë¶€ íŒë‹¨
        # 'rag_analysis_results'ëŠ” ë°±ì›Œë“œ í˜¸í™˜ìš©ìœ¼ë¡œ ì„ íƒ ì‚¬í•­ì´ë¯€ë¡œ ë¶€íŠ¸ ì¡°ê±´ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        required_tables = {'analysis_requests', 'repository_analyses'}
        existing_tables = set(inspector.get_table_names())
        print(f"ğŸ” í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {existing_tables}")
        return required_tables.issubset(existing_tables)
    except Exception as e:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ ë“± ì˜ˆì™¸ ë°œìƒ ì‹œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        print(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì´ˆê¸°í™” í•„ìš” ê°€ëŠ¥ì„±): {e}")
        return False

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°
    if _is_database_initialized():
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
        
    print("ğŸ”„ RAG Pipeline ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_connection():
        return False
    
    # Alembicì´ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” í…Œì´ë¸”ì„ ì§ì ‘ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # create_tables()
    
    # ì´ˆê¸°í™” ì™„ë£Œ í›„ ë‹¤ì‹œ í™•ì¸
    if not _is_database_initialized():
        print("âŒ ì´ˆê¸°í™” í›„ì—ë„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
        
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True

# ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ë°±ì›Œë“œ í˜¸í™˜ì„±)
def save_analysis_to_db(analysis_result):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        db = SessionLocal()
        
        # Git URL ì¶”ì¶œ (ì²« ë²ˆì§¸ ë ˆí¬ì§€í† ë¦¬ì˜ URL ì‚¬ìš©)
        git_url = ""
        if hasattr(analysis_result, 'repositories') and analysis_result.repositories:
            if hasattr(analysis_result.repositories[0], 'repository'):
                git_url = str(analysis_result.repositories[0].repository.url)
            else:
                git_url = "unknown"
        
        # í†µê³„ ê³„ì‚°
        total_files = 0
        total_lines = 0
        if hasattr(analysis_result, 'repositories') and analysis_result.repositories:
            total_files = sum(len(getattr(repo, 'files', [])) for repo in analysis_result.repositories)
            total_lines = sum(getattr(getattr(repo, 'code_metrics', None), 'lines_of_code', 0) for repo in analysis_result.repositories)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ìƒì„±
        db_record = RagAnalysisResult(
            analysis_id=analysis_result.analysis_id,
            git_url=git_url,
            analysis_date=analysis_result.created_at,
            status=analysis_result.status,
            repository_count=len(getattr(analysis_result, 'repositories', [])),
            total_files=total_files,
            total_lines_of_code=total_lines,
            repositories_data=json.dumps([repo.model_dump() if hasattr(repo, 'model_dump') else str(repo) for repo in getattr(analysis_result, 'repositories', [])], default=str, ensure_ascii=False),
            correlation_data=json.dumps(analysis_result.correlation_analysis.model_dump() if hasattr(analysis_result, 'correlation_analysis') and analysis_result.correlation_analysis else None, default=str, ensure_ascii=False),
            tech_specs_summary=json.dumps([], default=str, ensure_ascii=False),
            created_at=analysis_result.created_at,
            completed_at=getattr(analysis_result, 'completed_at', None),
            error_message=getattr(analysis_result, 'error_message', None)
        )
        
        # ê¸°ì¡´ ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        existing = db.query(RagAnalysisResult).filter(RagAnalysisResult.analysis_id == analysis_result.analysis_id).first()
        if existing:
            # ì—…ë°ì´íŠ¸
            for key, value in db_record.__dict__.items():
                if not key.startswith('_'):
                    setattr(existing, key, value)
            db.commit()
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {analysis_result.analysis_id}")
        else:
            # ìƒˆë¡œ ìƒì„±
            db.add(db_record)
            db.commit()
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {analysis_result.analysis_id}")
        
        db.close()
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        if 'db' in locals():
            db.rollback()
            db.close()
        return False
