"""Enhanced analysis API endpoints for tree-sitter, static analysis, and dependency analysis"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field

from analyzers.enhanced import EnhancedAnalyzer
from analyzers.git_analyzer import GitAnalyzer
from models.schemas import GitRepository, FileInfo

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/enhanced", tags=["Enhanced Analysis"])


class EnhancedAnalysisRequest(BaseModel):
    """Enhanced ë¶„ì„ ìš”ì²­ ëª¨ë¸"""
    repositories: List[GitRepository]
    include_tree_sitter: bool = Field(default=True, description="Tree-sitter AST ë¶„ì„ í¬í•¨ ì—¬ë¶€")
    include_static_analysis: bool = Field(default=True, description="ì •ì  ë¶„ì„ í¬í•¨ ì—¬ë¶€")
    include_dependency_analysis: bool = Field(default=True, description="ì˜ì¡´ì„± ë¶„ì„ í¬í•¨ ì—¬ë¶€")
    generate_report: bool = Field(default=True, description="ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€")


class EnhancedAnalysisResponse(BaseModel):
    """Enhanced ë¶„ì„ ì‘ë‹µ ëª¨ë¸"""
    analysis_id: str
    status: str
    message: str
    capabilities_available: Dict[str, Any]
    repositories: List[GitRepository]


class AnalysisStatusResponse(BaseModel):
    """ë¶„ì„ ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    analysis_id: str
    status: str
    progress: Dict[str, Any]
    results: Optional[Dict[str, Any]] = None
    report_path: Optional[str] = None


# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
enhanced_analyzer = EnhancedAnalyzer()
git_analyzer = GitAnalyzer()

# ë¶„ì„ ìƒíƒœ ì €ì¥ì†Œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
analysis_status = {}
analysis_results = {}


@router.get("/capabilities")
async def get_capabilities():
    """Enhanced ë¶„ì„ê¸°ì˜ ê¸°ëŠ¥ ìƒíƒœ ì¡°íšŒ"""
    try:
        capabilities = enhanced_analyzer.get_capabilities_status()
        return {
            "status": "success",
            "capabilities": capabilities,
            "message": "Enhanced analyzer capabilities retrieved successfully"
        }
    except Exception as e:
        logger.error(f"Error getting capabilities: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get capabilities: {str(e)}")


@router.post("/analyze", response_model=EnhancedAnalysisResponse)
async def start_enhanced_analysis(
    request: EnhancedAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """Enhanced ë¶„ì„ ì‹œì‘"""
    try:
        # ë¶„ì„ ID ìƒì„±
        analysis_id = f"enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(str(request.repositories)) % 10000}"
        
        # ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”
        analysis_status[analysis_id] = {
            "status": "started",
            "progress": {
                "current_step": "initializing",
                "completed_steps": [],
                "total_repositories": len(request.repositories),
                "processed_repositories": 0
            },
            "start_time": datetime.now().isoformat(),
            "repositories": request.repositories
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
        background_tasks.add_task(
            run_enhanced_analysis,
            analysis_id,
            request
        )
        
        # ê¸°ëŠ¥ ìƒíƒœ ì¡°íšŒ
        capabilities = enhanced_analyzer.get_capabilities_status()
        
        return EnhancedAnalysisResponse(
            analysis_id=analysis_id,
            status="started",
            message="Enhanced analysis started successfully",
            capabilities_available=capabilities,
            repositories=request.repositories
        )
        
    except Exception as e:
        logger.error(f"Error starting enhanced analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start analysis: {str(e)}")


@router.get("/status/{analysis_id}", response_model=AnalysisStatusResponse)
async def get_analysis_status(analysis_id: str):
    """ë¶„ì„ ìƒíƒœ ì¡°íšŒ"""
    try:
        if analysis_id not in analysis_status:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        status_info = analysis_status[analysis_id]
        results = analysis_results.get(analysis_id)
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
        report_path = None
        if results and status_info["status"] == "completed":
            report_dir = Path("output/enhanced_reports")
            report_file = report_dir / f"{analysis_id}_report.md"
            if report_file.exists():
                report_path = str(report_file)
        
        return AnalysisStatusResponse(
            analysis_id=analysis_id,
            status=status_info["status"],
            progress=status_info["progress"],
            results=results,
            report_path=report_path
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting analysis status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")


@router.get("/results/{analysis_id}")
async def get_analysis_results(analysis_id: str):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        if analysis_id not in analysis_results:
            raise HTTPException(status_code=404, detail="Analysis results not found")
        
        return {
            "analysis_id": analysis_id,
            "status": "success",
            "results": analysis_results[analysis_id]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting analysis results: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get results: {str(e)}")


@router.get("/report/{analysis_id}")
async def get_analysis_report(analysis_id: str):
    """ë¶„ì„ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    try:
        if analysis_id not in analysis_results:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
        report_dir = Path("output/enhanced_reports")
        report_file = report_dir / f"{analysis_id}_report.md"
        
        if not report_file.exists():
            raise HTTPException(status_code=404, detail="Report file not found")
        
        # ë¦¬í¬íŠ¸ ë‚´ìš© ì½ê¸°
        with open(report_file, 'r', encoding='utf-8') as f:
            report_content = f.read()
        
        return {
            "analysis_id": analysis_id,
            "status": "success",
            "report_content": report_content,
            "report_path": str(report_file)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting analysis report: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get report: {str(e)}")


@router.delete("/results/{analysis_id}")
async def delete_analysis_results(analysis_id: str):
    """ë¶„ì„ ê²°ê³¼ ì‚­ì œ"""
    try:
        # ë©”ëª¨ë¦¬ì—ì„œ ê²°ê³¼ ì‚­ì œ
        if analysis_id in analysis_status:
            del analysis_status[analysis_id]
        if analysis_id in analysis_results:
            del analysis_results[analysis_id]
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì‚­ì œ
        report_dir = Path("output/enhanced_reports")
        report_file = report_dir / f"{analysis_id}_report.md"
        if report_file.exists():
            report_file.unlink()
        
        return {
            "analysis_id": analysis_id,
            "status": "success",
            "message": "Analysis results deleted successfully"
        }
        
    except Exception as e:
        logger.error(f"Error deleting analysis results: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete results: {str(e)}")


@router.get("/list")
async def list_analyses():
    """ëª¨ë“  ë¶„ì„ ëª©ë¡ ì¡°íšŒ"""
    try:
        analyses = []
        for analysis_id, status_info in analysis_status.items():
            analyses.append({
                "analysis_id": analysis_id,
                "status": status_info["status"],
                "start_time": status_info["start_time"],
                "repositories_count": status_info["progress"]["total_repositories"],
                "processed_count": status_info["progress"]["processed_repositories"]
            })
        
        return {
            "status": "success",
            "total_analyses": len(analyses),
            "analyses": analyses
        }
        
    except Exception as e:
        logger.error(f"Error listing analyses: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list analyses: {str(e)}")


async def run_enhanced_analysis(analysis_id: str, request: EnhancedAnalysisRequest):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” Enhanced ë¶„ì„ í•¨ìˆ˜"""
    try:
        logger.info(f"Starting enhanced analysis {analysis_id}")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        analysis_status[analysis_id]["status"] = "running"
        analysis_status[analysis_id]["progress"]["current_step"] = "cloning_repositories"
        
        all_results = {
            "analysis_id": analysis_id,
            "repositories": [],
            "overall_summary": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # ê° ë ˆí¬ì§€í† ë¦¬ ë¶„ì„
        for i, repository in enumerate(request.repositories):
            try:
                logger.info(f"Processing repository {i+1}/{len(request.repositories)}: {repository.url}")
                
                # Git í´ë¡ 
                analysis_status[analysis_id]["progress"]["current_step"] = f"cloning_{repository.url}"
                clone_path = git_analyzer.clone_repository(repository)
                
                # íŒŒì¼ ëª©ë¡ ìƒì„±
                analysis_status[analysis_id]["progress"]["current_step"] = f"analyzing_files_{repository.url}"
                files = git_analyzer.analyze_repository_structure(clone_path)
                
                # Enhanced ë¶„ì„ ìˆ˜í–‰
                analysis_status[analysis_id]["progress"]["current_step"] = f"enhanced_analysis_{repository.url}"
                repo_results = enhanced_analyzer.analyze_repository(
                    clone_path=clone_path,
                    files=files,
                    include_tree_sitter=request.include_tree_sitter,
                    include_static_analysis=request.include_static_analysis,
                    include_dependency_analysis=request.include_dependency_analysis
                )
                
                # ê²°ê³¼ ì €ì¥
                repo_results["repository"] = repository.dict()
                all_results["repositories"].append(repo_results)
                
                # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                analysis_status[analysis_id]["progress"]["processed_repositories"] = i + 1
                analysis_status[analysis_id]["progress"]["completed_steps"].append(f"repository_{i+1}")
                
                logger.info(f"Completed analysis for repository: {repository.url}")
                
            except Exception as e:
                logger.error(f"Error analyzing repository {repository.url}: {e}")
                # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ë¥¸ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ê³„ì†
                continue
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        analysis_status[analysis_id]["progress"]["current_step"] = "generating_summary"
        all_results["overall_summary"] = generate_overall_summary(all_results["repositories"])
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        if request.generate_report:
            analysis_status[analysis_id]["progress"]["current_step"] = "generating_report"
            await generate_enhanced_report(analysis_id, all_results)
        
        # ê²°ê³¼ ì €ì¥
        analysis_results[analysis_id] = all_results
        
        # ìƒíƒœ ì™„ë£Œë¡œ ì—…ë°ì´íŠ¸
        analysis_status[analysis_id]["status"] = "completed"
        analysis_status[analysis_id]["progress"]["current_step"] = "completed"
        analysis_status[analysis_id]["end_time"] = datetime.now().isoformat()
        
        logger.info(f"Enhanced analysis {analysis_id} completed successfully")
        
    except Exception as e:
        logger.error(f"Enhanced analysis {analysis_id} failed: {e}")
        analysis_status[analysis_id]["status"] = "failed"
        analysis_status[analysis_id]["error"] = str(e)
        analysis_status[analysis_id]["end_time"] = datetime.now().isoformat()


def generate_overall_summary(repository_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ì „ì²´ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary = {
        "total_repositories": len(repository_results),
        "total_files": 0,
        "total_ast_nodes": 0,
        "total_code_issues": 0,
        "total_dependencies": 0,
        "total_vulnerabilities": 0,
        "languages_detected": set(),
        "tools_used": set(),
        "capabilities_summary": {
            "tree_sitter_used": 0,
            "static_analysis_used": 0,
            "dependency_analysis_used": 0
        }
    }
    
    for repo_result in repository_results:
        summary["total_files"] += repo_result.get("total_files", 0)
        
        # Tree-sitter ìš”ì•½
        if "tree_sitter" in repo_result.get("summary", {}):
            ts_summary = repo_result["summary"]["tree_sitter"]
            summary["total_ast_nodes"] += ts_summary.get("total_nodes", 0)
            summary["languages_detected"].update(ts_summary.get("languages", {}).keys())
            summary["capabilities_summary"]["tree_sitter_used"] += 1
        
        # ì •ì  ë¶„ì„ ìš”ì•½
        if "static_analysis" in repo_result.get("summary", {}):
            static_summary = repo_result["summary"]["static_analysis"]
            summary["total_code_issues"] += static_summary.get("total_issues", 0)
            summary["tools_used"].update(static_summary.get("tools_used", []))
            summary["capabilities_summary"]["static_analysis_used"] += 1
        
        # ì˜ì¡´ì„± ë¶„ì„ ìš”ì•½
        if "dependency_analysis" in repo_result.get("summary", {}):
            dep_summary = repo_result["summary"]["dependency_analysis"]
            summary["total_dependencies"] += dep_summary.get("total_dependencies", 0)
            summary["total_vulnerabilities"] += dep_summary.get("total_vulnerabilities", 0)
            summary["capabilities_summary"]["dependency_analysis_used"] += 1
    
    # Setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    summary["languages_detected"] = list(summary["languages_detected"])
    summary["tools_used"] = list(summary["tools_used"])
    
    return summary


async def generate_enhanced_report(analysis_id: str, results: Dict[str, Any]):
    """Enhanced ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    try:
        # ë¦¬í¬íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        report_dir = Path("output/enhanced_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„±
        report_lines = []
        report_lines.append(f"# ğŸ” Enhanced Analysis Report - {analysis_id}")
        report_lines.append("")
        report_lines.append(f"**Analysis Date:** {results['timestamp']}")
        report_lines.append(f"**Total Repositories:** {len(results['repositories'])}")
        report_lines.append("")
        
        # ì „ì²´ ìš”ì•½
        overall = results.get("overall_summary", {})
        if overall:
            report_lines.append("## ğŸ“Š Overall Summary")
            report_lines.append("")
            report_lines.append(f"- **Total Files Analyzed:** {overall.get('total_files', 0)}")
            report_lines.append(f"- **Total AST Nodes:** {overall.get('total_ast_nodes', 0)}")
            report_lines.append(f"- **Total Code Issues:** {overall.get('total_code_issues', 0)}")
            report_lines.append(f"- **Total Dependencies:** {overall.get('total_dependencies', 0)}")
            report_lines.append(f"- **Total Vulnerabilities:** {overall.get('total_vulnerabilities', 0)}")
            
            languages = overall.get('languages_detected', [])
            if languages:
                report_lines.append(f"- **Languages Detected:** {', '.join(languages)}")
            
            tools = overall.get('tools_used', [])
            if tools:
                report_lines.append(f"- **Analysis Tools Used:** {', '.join(tools)}")
            
            report_lines.append("")
        
        # ê° ë ˆí¬ì§€í† ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
        for i, repo_result in enumerate(results['repositories'], 1):
            repo_info = repo_result.get('repository', {})
            report_lines.append(f"## ğŸ“ Repository {i}: {repo_info.get('name', 'Unknown')}")
            report_lines.append("")
            report_lines.append(f"**URL:** {repo_info.get('url', 'Unknown')}")
            report_lines.append(f"**Branch:** {repo_info.get('branch', 'main')}")
            report_lines.append("")
            
            # ê°œë³„ ë ˆí¬ì§€í† ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±
            individual_report = enhanced_analyzer.generate_comprehensive_report(repo_result)
            # í—¤ë” ë ˆë²¨ ì¡°ì • (## -> ###)
            individual_report = individual_report.replace("## ", "### ")
            individual_report = individual_report.replace("# ğŸ” Enhanced Code Analysis Report", "")
            report_lines.append(individual_report)
            report_lines.append("")
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_content = "\n".join(report_lines)
        report_file = report_dir / f"{analysis_id}_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"Enhanced analysis report generated: {report_file}")
        
    except Exception as e:
        logger.error(f"Error generating enhanced report: {e}")
        raise