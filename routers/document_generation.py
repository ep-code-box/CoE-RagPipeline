from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends, Query
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid
import logging
import os

from models.schemas import (
    DocumentGenerationRequest,
    DocumentGenerationResponse,
    DocumentGenerationStatus,
    GeneratedDocument
)
from services.llm_service import LLMDocumentService, DocumentType
from services.analysis_service import AnalysisService
from core.database import get_db

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/api/v1/documents", 
    tags=["ğŸ“„ Document Generation"],
    responses={
        200: {"description": "ë¬¸ì„œ ìƒì„± ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        404: {"description": "ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)

# ë¬¸ì„œ ìƒì„± ì‘ì—… ìƒíƒœ ì¶”ì 
document_generation_tasks = {}

@router.post(
    "/generate",
    response_model=DocumentGenerationResponse,
    summary="ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë¬¸ì„œ ìƒì„±",
    description="""
    **ë¶„ì„ ì™„ë£Œëœ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë¬¸ì„œë¥¼ LLMìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.**
    
    ### ğŸ“„ ìƒì„± ê°€ëŠ¥í•œ ë¬¸ì„œ íƒ€ì…
    - **development_guide**: ê°œë°œ ê°€ì´ë“œ (ì½”ë”© ì»¨ë²¤ì…˜, ëª¨ë²” ì‚¬ë¡€)
    - **api_documentation**: API ë¬¸ì„œ (ì—”ë“œí¬ì¸íŠ¸, ì‚¬ìš©ë²•)
    - **architecture_overview**: ì•„í‚¤í…ì²˜ ê°œìš” (ì‹œìŠ¤í…œ êµ¬ì¡°, ì»´í¬ë„ŒíŠ¸)
    - **code_review_summary**: ì½”ë“œ ë¦¬ë·° ìš”ì•½ (ì´ìŠˆ, ê°œì„ ì‚¬í•­)
    - **technical_specification**: ê¸°ìˆ  ëª…ì„¸ì„œ (ê¸°ìˆ  ìŠ¤íƒ, ì˜ì¡´ì„±)
    - **deployment_guide**: ë°°í¬ ê°€ì´ë“œ (í™˜ê²½ ì„¤ì •, ë°°í¬ ê³¼ì •)
    - **troubleshooting_guide**: ë¬¸ì œ í•´ê²° ê°€ì´ë“œ (ì¼ë°˜ì  ì˜¤ë¥˜, í•´ê²°ë²•)
    
    ### ğŸŒ ì§€ì› ì–¸ì–´
    - **korean**: í•œêµ­ì–´ (ê¸°ë³¸ê°’)
    - **english**: ì˜ì–´
    
    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    ```bash
    curl -X POST "http://localhost:8001/api/v1/documents/generate" \\
      -H "Content-Type: application/json" \\
      -d '{
        "analysis_id": "3cbf3db0-fd9e-410c-bdaa-30cdeb9d7d6c",
        "document_types": ["development_guide", "api_documentation"],
        "language": "korean",
        "custom_prompt": "íŠ¹íˆ FastAPI ê´€ë ¨ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
      }'
    ```
    
    ### âš¡ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
    - ë¬¸ì„œ ìƒì„±ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤
    - ìƒì„± ìƒíƒœëŠ” `/status/{task_id}` ì—”ë“œí¬ì¸íŠ¸ë¡œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤
    - ì™„ë£Œëœ ë¬¸ì„œëŠ” `output/documents/{analysis_id}/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
    """
)
async def generate_documents(
    request: DocumentGenerationRequest,
    background_tasks: BackgroundTasks,
    use_source_summaries: bool = Query(True, description="ì†ŒìŠ¤ì½”ë“œ ìš”ì•½ ì‚¬ìš© ì—¬ë¶€"),
    db: Session = Depends(get_db)
):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ë¶„ì„ ê²°ê³¼ ì¡´ì¬ í™•ì¸
        analysis_service = AnalysisService()
        analysis_result = analysis_service.load_analysis_result(request.analysis_id)
        
        if not analysis_result:
            raise HTTPException(
                status_code=404,
                detail={
                    "message": "ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "analysis_id": request.analysis_id,
                    "suggestion": "ë¨¼ì € /api/v1/analyze ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."
                }
            )
        
        # ì‘ì—… ID ìƒì„±
        task_id = str(uuid.uuid4())
        
        # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
        document_generation_tasks[task_id] = {
            "status": DocumentGenerationStatus.PENDING,
            "analysis_id": request.analysis_id,
            "document_types": request.document_types,
            "language": request.language,
            "created_at": datetime.now(),
            "completed_at": None,
            "error_message": None,
            "generated_documents": []
        }
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬¸ì„œ ìƒì„± ì‹¤í–‰
        background_tasks.add_task(
            _generate_documents_background,
            task_id,
            request,
            analysis_result,
            use_source_summaries
        )
        
        logger.info(f"Document generation task started: {task_id} for analysis {request.analysis_id}")
        
        return DocumentGenerationResponse(
            task_id=task_id,
            status=DocumentGenerationStatus.PENDING,
            message="ë¬¸ì„œ ìƒì„±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            analysis_id=request.analysis_id,
            document_types=request.document_types,
            language=request.language
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document generation request failed: {e}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get(
    "/status/{task_id}",
    response_model=DocumentGenerationResponse,
    summary="ë¬¸ì„œ ìƒì„± ìƒíƒœ ì¡°íšŒ",
    description="""
    **ë¬¸ì„œ ìƒì„± ì‘ì—…ì˜ í˜„ì¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.**
    
    ### ğŸ“Š ìƒíƒœ ì •ë³´
    - **pending**: ëŒ€ê¸° ì¤‘
    - **running**: ìƒì„± ì¤‘
    - **completed**: ì™„ë£Œ
    - **failed**: ì‹¤íŒ¨
    
    ### ğŸ“„ ì™„ë£Œ ì‹œ ì œê³µ ì •ë³´
    - ìƒì„±ëœ ë¬¸ì„œ ëª©ë¡
    - ê° ë¬¸ì„œì˜ íŒŒì¼ ê²½ë¡œ
    - í† í° ì‚¬ìš©ëŸ‰
    - ìƒì„± ì‹œê°„
    """
)
async def get_document_generation_status(task_id: str):
    """ë¬¸ì„œ ìƒì„± ì‘ì—… ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if task_id not in document_generation_tasks:
        raise HTTPException(
            status_code=404,
            detail={
                "message": "ë¬¸ì„œ ìƒì„± ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "task_id": task_id,
                "available_tasks": list(document_generation_tasks.keys())
            }
        )
    
    task_info = document_generation_tasks[task_id]
    
    return DocumentGenerationResponse(
        task_id=task_id,
        status=task_info["status"],
        message=_get_status_message(task_info["status"]),
        analysis_id=task_info["analysis_id"],
        document_types=task_info["document_types"],
        language=task_info["language"],
        created_at=task_info["created_at"],
        completed_at=task_info.get("completed_at"),
        error_message=task_info.get("error_message"),
        generated_documents=task_info.get("generated_documents", [])
    )


@router.get(
    "/types",
    response_model=List[str],
    summary="ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ íƒ€ì… ëª©ë¡",
    description="""
    **ìƒì„± ê°€ëŠ¥í•œ ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.**
    
    ê° ë¬¸ì„œ íƒ€ì…ì€ íŠ¹ì • ëª©ì ê³¼ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°,
    ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë‚´ìš©ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
    """
)
async def get_available_document_types():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ íƒ€ì… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        llm_service = LLMDocumentService()
        return llm_service.get_available_document_types()
    except Exception as e:
        logger.error(f"Failed to get document types: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ íƒ€ì… ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.get(
    "/list/{analysis_id}",
    response_model=List[GeneratedDocument],
    summary="ë¶„ì„ë³„ ìƒì„±ëœ ë¬¸ì„œ ëª©ë¡",
    description="""
    **íŠ¹ì • ë¶„ì„ IDì— ëŒ€í•´ ìƒì„±ëœ ëª¨ë“  ë¬¸ì„œì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.**
    
    ### ğŸ“ ë¬¸ì„œ ì €ì¥ ìœ„ì¹˜
    - ê²½ë¡œ: `output/documents/{analysis_id}/`
    - íŒŒì¼ëª… í˜•ì‹: `{document_type}_{language}.md`
    
    ### ğŸ“„ ë°˜í™˜ ì •ë³´
    - ë¬¸ì„œ íƒ€ì…
    - íŒŒì¼ ê²½ë¡œ
    - ìƒì„± ì‹œê°„
    - íŒŒì¼ í¬ê¸°
    - ì–¸ì–´
    """
)
async def list_generated_documents(analysis_id: str):
    """íŠ¹ì • ë¶„ì„ IDì— ëŒ€í•´ ìƒì„±ëœ ë¬¸ì„œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        documents_dir = f"output/documents/{analysis_id}"
        
        if not os.path.exists(documents_dir):
            return []
        
        documents = []
        for filename in os.listdir(documents_dir):
            if filename.endswith('.md'):
                file_path = os.path.join(documents_dir, filename)
                file_stat = os.stat(file_path)
                
                # íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì…ê³¼ ì–¸ì–´ ì¶”ì¶œ
                name_parts = filename[:-3].split('_')  # .md ì œê±°
                document_type = name_parts[0] if len(name_parts) > 0 else "unknown"
                language = name_parts[1] if len(name_parts) > 1 else "korean"
                
                documents.append(GeneratedDocument(
                    document_type=document_type,
                    language=language,
                    file_path=file_path,
                    file_size=file_stat.st_size,
                    created_at=datetime.fromtimestamp(file_stat.st_ctime),
                    analysis_id=analysis_id
                ))
        
        return documents
        
    except Exception as e:
        logger.error(f"Failed to list documents for analysis {analysis_id}: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.delete(
    "/delete/{analysis_id}",
    summary="ë¶„ì„ë³„ ìƒì„±ëœ ë¬¸ì„œ ì‚­ì œ",
    description="""
    **íŠ¹ì • ë¶„ì„ IDì— ëŒ€í•´ ìƒì„±ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.**
    
    ### âš ï¸ ì£¼ì˜ì‚¬í•­
    - ì‚­ì œëœ ë¬¸ì„œëŠ” ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
    - ë””ë ‰í† ë¦¬ ì „ì²´ê°€ ì‚­ì œë©ë‹ˆë‹¤
    """
)
async def delete_generated_documents(analysis_id: str):
    """íŠ¹ì • ë¶„ì„ IDì— ëŒ€í•´ ìƒì„±ëœ ë¬¸ì„œë“¤ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        documents_dir = f"output/documents/{analysis_id}"
        
        if not os.path.exists(documents_dir):
            raise HTTPException(
                status_code=404,
                detail=f"ë¶„ì„ ID {analysis_id}ì— ëŒ€í•œ ë¬¸ì„œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )
        
        import shutil
        shutil.rmtree(documents_dir)
        
        logger.info(f"Deleted documents directory: {documents_dir}")
        
        return {"message": f"ë¶„ì„ ID {analysis_id}ì˜ ëª¨ë“  ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete documents for analysis {analysis_id}: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


async def _generate_documents_background(
    task_id: str,
    request: DocumentGenerationRequest,
    analysis_result: Any,
    use_source_summaries: bool = True
):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬¸ì„œ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì‘ì—… ìƒíƒœë¥¼ ì‹¤í–‰ ì¤‘ìœ¼ë¡œ ë³€ê²½
        document_generation_tasks[task_id]["status"] = DocumentGenerationStatus.RUNNING
        
        logger.info(f"Starting document generation for task {task_id}")
        
        # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        llm_service = LLMDocumentService()
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ê°œì„ ëœ ë¡œì§)
        analysis_data = {
            "analysis_id": request.analysis_id,
            "repositories": [],
            "tech_specs": [],
            "ast_analysis": {},
            "code_metrics": {}
        }
        
        logger.info(f"Processing analysis result for {request.analysis_id}")
        logger.debug(f"Analysis result type: {type(analysis_result)}")
        logger.debug(f"Analysis result has repositories: {hasattr(analysis_result, 'repositories')}")
        
        # ì €ì¥ì†Œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
        if hasattr(analysis_result, 'repositories') and analysis_result.repositories:
            logger.info(f"Found {len(analysis_result.repositories)} repositories in analysis result")
            
            for i, repo in enumerate(analysis_result.repositories):
                logger.debug(f"Processing repository {i}: {type(repo)}")
                
                # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                repo_data = {
                    "url": "Unknown",
                    "branch": "main", 
                    "name": None
                }
                
                # repository ì •ë³´ ì¶”ì¶œ
                if hasattr(repo, 'repository') and repo.repository:
                    repo_data["url"] = str(repo.repository.url) if hasattr(repo.repository, 'url') else "Unknown"
                    repo_data["branch"] = repo.repository.branch if hasattr(repo.repository, 'branch') else "main"
                    repo_data["name"] = repo.repository.name if hasattr(repo.repository, 'name') else None
                    logger.debug(f"Repository info: {repo_data}")
                
                analysis_data["repositories"].append(repo_data)
                
                # ê¸°ìˆ  ìŠ¤í™ ì •ë³´ ì¶”ê°€ (ê°œì„ ëœ ë¡œì§)
                if hasattr(repo, 'tech_specs') and repo.tech_specs:
                    logger.debug(f"Found {len(repo.tech_specs)} tech specs for repository {i}")
                    
                    for j, tech_spec in enumerate(repo.tech_specs):
                        tech_data = {
                            "name": getattr(tech_spec, 'language', 'Unknown'),
                            "version": getattr(tech_spec, 'version', 'Unknown'),
                            "framework": getattr(tech_spec, 'framework', None),
                            "dependencies": getattr(tech_spec, 'dependencies', [])
                        }
                        analysis_data["tech_specs"].append(tech_data)
                        logger.debug(f"Tech spec {j}: {tech_data}")
                
                # AST ë¶„ì„ ì •ë³´ ì¶”ê°€ (ê°œì„ ëœ ë¡œì§)
                if hasattr(repo, 'ast_analysis') and repo.ast_analysis:
                    logger.debug(f"Found AST analysis data for repository {i}")
                    if isinstance(repo.ast_analysis, dict):
                        analysis_data["ast_analysis"].update(repo.ast_analysis)
                    else:
                        logger.warning(f"AST analysis data is not a dict: {type(repo.ast_analysis)}")
                
                # ì½”ë“œ ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€ (ê°œì„ ëœ ë¡œì§)
                if hasattr(repo, 'code_metrics') and repo.code_metrics:
                    logger.debug(f"Found code metrics for repository {i}")
                    metrics = repo.code_metrics
                    
                    # CodeMetrics ëª¨ë¸ì˜ ì‹¤ì œ í•„ë“œëª… ì‚¬ìš©
                    analysis_data["code_metrics"] = {
                        "lines_of_code": getattr(metrics, 'lines_of_code', 0),
                        "cyclomatic_complexity": getattr(metrics, 'cyclomatic_complexity', 0),
                        "maintainability_index": getattr(metrics, 'maintainability_index', 0),
                        "comment_ratio": getattr(metrics, 'comment_ratio', 0)
                    }
                    logger.debug(f"Code metrics: {analysis_data['code_metrics']}")
        else:
            logger.warning(f"No repositories found in analysis result for {request.analysis_id}")
        
        # ìµœì¢… ë¶„ì„ ë°ì´í„° ë¡œê¹…
        logger.info(f"Final analysis data summary:")
        logger.info(f"  - Repositories: {len(analysis_data['repositories'])}")
        logger.info(f"  - Tech specs: {len(analysis_data['tech_specs'])}")
        logger.info(f"  - AST analysis entries: {len(analysis_data['ast_analysis'])}")
        logger.info(f"  - Code metrics: {bool(analysis_data['code_metrics'])}")
        
        # ë¬¸ì„œ íƒ€ì…ì„ DocumentType enumìœ¼ë¡œ ë³€í™˜
        document_types = [DocumentType(doc_type) for doc_type in request.document_types]
        
        # ë¬¸ì„œ ìƒì„± (ì†ŒìŠ¤ì½”ë“œ ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ)
        if use_source_summaries:
            logger.info(f"Generating documents with source summaries for analysis {request.analysis_id}")
            generated_documents = await llm_service.generate_documents_with_source_summaries(
                analysis_data=analysis_data,
                analysis_id=request.analysis_id,
                document_types=document_types,
                language=request.language,
                custom_prompt=request.custom_prompt
            )
        else:
            logger.info(f"Generating documents without source summaries for analysis {request.analysis_id}")
            generated_documents = await llm_service.generate_multiple_documents(
                analysis_data=analysis_data,
                document_types=document_types,
                language=request.language
            )
        
        # ìƒì„±ëœ ë¬¸ì„œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥
        documents_dir = f"output/documents/{request.analysis_id}"
        os.makedirs(documents_dir, exist_ok=True)
        
        saved_documents = []
        for doc in generated_documents:
            if "error" not in doc:  # ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ ë¬¸ì„œë§Œ ì €ì¥
                doc_filename = f"{doc['document_type']}_{doc['language']}.md"
                doc_path = os.path.join(documents_dir, doc_filename)
                
                with open(doc_path, 'w', encoding='utf-8') as f:
                    f.write(doc['content'])
                
                # ì €ì¥ëœ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                file_stat = os.stat(doc_path)
                saved_documents.append(GeneratedDocument(
                    document_type=doc['document_type'],
                    language=doc['language'],
                    file_path=doc_path,
                    file_size=file_stat.st_size,
                    created_at=datetime.fromtimestamp(file_stat.st_ctime),
                    analysis_id=request.analysis_id,
                    tokens_used=doc.get('tokens_used', 0)
                ))
                
                logger.info(f"Document saved: {doc_path}")
            else:
                logger.error(f"Failed to generate document {doc['document_type']}: {doc.get('error')}")
        
        # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        document_generation_tasks[task_id].update({
            "status": DocumentGenerationStatus.COMPLETED,
            "completed_at": datetime.now(),
            "generated_documents": saved_documents
        })
        
        logger.info(f"Document generation completed for task {task_id}. Generated {len(saved_documents)} documents.")
        
    except Exception as e:
        logger.error(f"Document generation failed for task {task_id}: {e}")
        
        # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
        document_generation_tasks[task_id].update({
            "status": DocumentGenerationStatus.FAILED,
            "completed_at": datetime.now(),
            "error_message": str(e)
        })


def _get_status_message(status: DocumentGenerationStatus) -> str:
    """ìƒíƒœì— ë”°ë¥¸ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    messages = {
        DocumentGenerationStatus.PENDING: "ë¬¸ì„œ ìƒì„± ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.",
        DocumentGenerationStatus.RUNNING: "ë¬¸ì„œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        DocumentGenerationStatus.COMPLETED: "ë¬¸ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        DocumentGenerationStatus.FAILED: "ë¬¸ì„œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    }
    return messages.get(status, "ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤.")